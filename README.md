# âœ¨ AI-Based Story Generator  

## ğŸ“Œ Overview  
The **AI-Based Story Generator** is a web application that generates creative short stories based on user-provided prompts. It uses a **fine-tuned Seq2Seq model** trained on the **Writing Prompts dataset** to produce engaging and contextually relevant narratives.  

This application is designed for **writers, hobbyists, and AI enthusiasts** to experiment with AI-generated storytelling using an intuitive web interface.  

---

## ğŸ¯ Features  
âœ”ï¸ **AI-Powered Story Generation** â€“ Creates unique, engaging stories from user prompts.  
âœ”ï¸ **Fine-Tuned Seq2Seq Model** â€“ Improves coherence, creativity, and fluency.  
âœ”ï¸ **Web-Based Interface** â€“ Built with Streamlit for easy use.  
âœ”ï¸ **Customizable Creativity Settings** â€“ Adjust temperature, top-k, and top-p.  
âœ”ï¸ **Story Management** â€“ Save, copy, or share generated stories.  
âœ”ï¸ **GPU Acceleration Support** â€“ Optional for faster inference.  

---

## ğŸ›  Setup & Installation  

### ğŸ”¹ Clone the Repository  
```bash
git clone https://github.com/vishwanath090/Storygeneration
cd storygenerator
ğŸ”¹ Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt
ğŸ“‚ Dataset: Writing Prompts
The Writing Prompts dataset is a large-scale dataset designed for creative writing and storytelling. It contains diverse prompts paired with user-generated responses, making it an excellent resource for training AI models.

ğŸ”¹ Dataset Structure
train.wp.source â€“ Training set prompts
train.wp.target â€“ Corresponding training stories
valid.wp.source â€“ Validation set prompts
valid.wp.target â€“ Corresponding validation stories
test.wp.source â€“ Test set prompts
test.wp.target â€“ Corresponding test stories
ğŸ“Œ Model Training & Fine-Tuning
The model is fine-tuned on the Writing Prompts dataset using Hugging Face's Transformers library.

ğŸ”¹ Training Steps
1ï¸âƒ£ Preprocess the Dataset â€“ Clean and tokenize the data.
2ï¸âƒ£ Fine-Tune the Model â€“ Use the command below:

bash
Copy
Edit
python train.py --model_name seq2seq --epochs 3 --batch_size 8
3ï¸âƒ£ Save the Trained Model â€“ The trained model is stored for later inference.

ğŸš€ Running the Web App
ğŸ”¹ Start the Application
bash
Copy
Edit
streamlit run app.py
ğŸ”¹ Access the Web Interface
After running the command, open your browser and navigate to the Streamlit-provided URL.

ğŸ”¹ Usage
1ï¸âƒ£ Enter a Writing Prompt â€“ Provide a creative idea or sentence.
2ï¸âƒ£ Click "Generate" â€“ The AI creates a unique story.
3ï¸âƒ£ Adjust Creativity Parameters â€“ Modify temperature and top-k settings.
4ï¸âƒ£ Save, Copy, or Share â€“ Manage generated stories easily.

ğŸ“Š Model Evaluation
ğŸ”¹ Performance Metrics
bash
Copy
Edit
ğŸ“Œ Accuracy â€“ Measures token-level correctness  
ğŸ“Œ F1 Score (weighted) â€“ Evaluates overall performance  
ğŸ”¹ Evaluate Model Performance
bash
Copy
Edit
python evaluate.py
âš™ï¸ Technologies Used
âœ”ï¸ Backend â€“ Python, TensorFlow/PyTorch, Hugging Face Transformers
âœ”ï¸ Frontend â€“ Streamlit, HTML, CSS, JavaScript
âœ”ï¸ Model Architecture â€“ Seq2Seq for text generation
âœ”ï¸ Deployment â€“ Local execution (future cloud-based options)

ğŸŒŸ Future Enhancements
bash
Copy
Edit
ğŸš€ Implement Transformer models (GPT, BERT)  
ğŸš€ Improve dataset preprocessing  
ğŸš€ Optimize model training for better results  
ğŸš€ Support multiple story genres for personalized storytelling  
ğŸš€ Develop a mobile-friendly UI  
ğŸš€ Deploy as a cloud-based service  
